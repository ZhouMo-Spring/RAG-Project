"""
æ•´åˆçš„RAGç³»ç»Ÿ
å°†æ£€ç´¢ã€å‘é‡åŒ–ã€LLMè°ƒç”¨ç­‰åŠŸèƒ½æ•´åˆåœ¨ä¸€ä¸ªç±»ä¸­ï¼Œç®€åŒ–è°ƒç”¨å…³ç³»
æ”¯æŒå‘é‡åº“æŒä¹…åŒ–å­˜å‚¨
"""

import os
import pickle
import hashlib
from typing import List, Optional, Dict, Any
from langchain_core.documents import Document
from langchain_community.document_loaders import (
    DirectoryLoader, PyPDFLoader, UnstructuredWordDocumentLoader,
    TextLoader, CSVLoader, UnstructuredHTMLLoader, MHTMLLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema.retriever import BaseRetriever
from langchain_huggingface import HuggingFaceEmbeddings
from config.config import Config
from llm_client import create_llm_client, BaseLLMClient


class RAGSystem:
    """æ•´åˆçš„RAGç³»ç»Ÿç±»"""
    
    def __init__(self, user_id: Optional[str] = None):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            user_id: ç”¨æˆ·IDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€çŸ¥è¯†åº“
        """
        self.user_id = user_id
        self.config = Config.get_instance()
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self._init_embedding()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self._init_llm_client()
        
        # åˆå§‹åŒ–è·¯å¾„
        self._init_paths()
        
        # å­˜å‚¨å‘é‡åº“
        self._vectorstores: Dict[str, FAISS] = {}
        self._retrievers: Dict[str, BaseRetriever] = {}
        
        # å°è¯•åŠ è½½ç¼“å­˜çš„å‘é‡åº“ï¼Œå¦‚æœå¤±è´¥åˆ™é‡æ–°æ„å»º
        if not self._load_cached_vectorstore():
            self._build_vectorstore()
    
    def _init_embedding(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        embedding_model_name = self.config.get_with_nested_params(
            "model", "embedding", "model-name"
        )
        device = self.config.get_with_nested_params(
            "model", "embedding", "device"
        )
        
        self.embedding = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        print(f"âœ… åˆå§‹åŒ–embeddingæ¨¡å‹: {embedding_model_name}")
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.llm_client = create_llm_client()
        print("âœ… åˆå§‹åŒ–LLMå®¢æˆ·ç«¯")
    
    def _init_paths(self):
        """åˆå§‹åŒ–è·¯å¾„"""
        if self.user_id:
            # ç”¨æˆ·ç‰¹å®šè·¯å¾„
            self.data_path = os.path.join("user_data", self.user_id)
            if not os.path.exists(self.data_path):
                os.makedirs(self.data_path)
                print(f"ğŸ“ åˆ›å»ºç”¨æˆ·æ–‡ä»¶å¤¹: {self.data_path}")
        else:
            # å…¨å±€è·¯å¾„
            self.data_path = self.config.get_with_nested_params("Knowledge-base-path")
            if not os.path.exists(self.data_path):
                os.makedirs(self.data_path)
                print(f"ğŸ“ åˆ›å»ºçŸ¥è¯†åº“æ–‡ä»¶å¤¹: {self.data_path}")
    
    def _load_documents(self) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        print(f"ğŸ“š ä» {self.data_path} åŠ è½½æ–‡æ¡£...")
        
        # ç»Ÿè®¡ä¿¡æ¯
        file_count = 0
        page_count = 0
        all_docs = []
        
        # åŠ è½½å„ç§æ ¼å¼çš„æ–‡æ¡£
        loaders = [
            DirectoryLoader(self.data_path, glob="**/*.pdf", loader_cls=PyPDFLoader, silent_errors=True),
            DirectoryLoader(self.data_path, glob="**/*.docx", loader_cls=UnstructuredWordDocumentLoader, silent_errors=True),
            DirectoryLoader(self.data_path, glob="**/*.txt", loader_cls=TextLoader, silent_errors=True, loader_kwargs={"autodetect_encoding": True}),
            DirectoryLoader(self.data_path, glob="**/*.csv", loader_cls=CSVLoader, silent_errors=True, loader_kwargs={"autodetect_encoding": True}),
            DirectoryLoader(self.data_path, glob="**/*.html", loader_cls=UnstructuredHTMLLoader, silent_errors=True),
            DirectoryLoader(self.data_path, glob="**/*.mhtml", loader_cls=MHTMLLoader, silent_errors=True),
            DirectoryLoader(self.data_path, glob="**/*.md", loader_cls=UnstructuredMarkdownLoader, silent_errors=True),
        ]
        
        for loader in loaders:
            try:
                docs = loader.load()
                if docs:
                    # ç»Ÿè®¡æ–‡ä»¶æ•°é‡ï¼ˆé€šè¿‡æ–‡ä»¶åå»é‡ï¼‰
                    unique_files = set()
                    for doc in docs:
                        # æ­£ç¡®è®¿é—®metadata
                        if 'source' in doc.metadata:
                            unique_files.add(doc.metadata['source'])
                        elif 'file_path' in doc.metadata:
                            unique_files.add(doc.metadata['file_path'])
                        elif 'file_name' in doc.metadata:
                            unique_files.add(doc.metadata['file_name'])
                    
                    file_count += len(unique_files)
                    page_count += len(docs)
                    all_docs.extend(docs)
                        
            except Exception as e:
                print(f"  - åŠ è½½æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ“Š æ€»å…±åŠ è½½äº† {file_count} ä¸ªæ–‡ä»¶ï¼Œ{page_count} ä¸ªé¡µé¢/ç‰‡æ®µ")
        return all_docs
    
    def _build_vectorstore(self):
        """æ„å»ºå‘é‡åº“"""
        print("ğŸ”¨ æ„å»ºå‘é‡åº“...")
        
        # åŠ è½½æ–‡æ¡£
        docs = self._load_documents()
        
        if not docs:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œå‘é‡åº“æ„å»ºå¤±è´¥")
            return
        
        # åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000, chunk_overlap=100
        )
        splits = text_splitter.split_documents(docs)
        print(f"âœ‚ï¸  æ–‡æ¡£åˆ†å‰²ä¸º {len(splits)} ä¸ªç‰‡æ®µ")
        
        # åˆ›å»ºå‘é‡åº“
        vectorstore = FAISS.from_documents(documents=splits, embedding=self.embedding)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
        
        # å­˜å‚¨å‘é‡åº“å’Œæ£€ç´¢å™¨
        key = self.user_id or "global"
        self._vectorstores[key] = vectorstore
        self._retrievers[key] = retriever
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_vectorstore_cache(vectorstore, splits)
        
        print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆ (key: {key})")
    
    def retrieve_documents(self, query: str) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        key = self.user_id or "global"
        retriever = self._retrievers.get(key)
        
        if not retriever:
            print("âŒ å‘é‡åº“æœªæ„å»ºï¼Œæ— æ³•æ£€ç´¢")
            return []
        
        try:
            docs = retriever.invoke(query)
            print(f"ğŸ” æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            return docs
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def format_documents(self, docs: List[Document]):
        """
        è¿”å›ä¸¤ä¸ªå†…å®¹ï¼š
        - formatted_docs: ç”¨äºpromptçš„å®Œæ•´å†…å®¹ï¼ˆå«æ­£æ–‡ï¼‰
        - doc_infos: ä»…åŒ…å«doc_infoï¼ˆä¸å«æ­£æ–‡ï¼‰
        """
        if not docs:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£ä¿¡æ¯", []

        formatted_docs = []
        doc_infos = []
        for i, doc in enumerate(docs, 1):
            # è·å–æ–‡æ¡£æ¥æºä¿¡æ¯
            source = "æœªçŸ¥æ¥æº"
            page = "æœªçŸ¥é¡µç "
            
            # æ­£ç¡®è®¿é—®metadata
            if 'source' in doc.metadata:
                source = doc.metadata['source']
            elif 'file_path' in doc.metadata:
                source = doc.metadata['file_path']
            elif 'file_name' in doc.metadata:
                source = doc.metadata['file_name']
            
            if 'page' in doc.metadata:
                # é¡µç ä»1å¼€å§‹
                page_num = doc.metadata['page']
                if isinstance(page_num, int):
                    page = str(page_num + 1)  # ä»0å¼€å§‹è½¬æ¢ä¸ºä»1å¼€å§‹
                else:
                    page = str(page_num)
            elif 'page_number' in doc.metadata:
                # é¡µç ä»1å¼€å§‹
                page_num = doc.metadata['page_number']
                if isinstance(page_num, int):
                    page = str(page_num + 1)  # ä»0å¼€å§‹è½¬æ¢ä¸ºä»1å¼€å§‹
                else:
                    page = str(page_num)
            elif 'row' in doc.metadata:
                # è¡Œæ•°ä»1å¼€å§‹
                page_num = doc.metadata['row']
                if isinstance(page_num, int):
                    page = str(page_num + 1)  # ä»0å¼€å§‹è½¬æ¢ä¸ºä»1å¼€å§‹
                else:
                    page = str(page_num)        
            
            # æå–æ–‡ä»¶å
            if source != "æœªçŸ¥æ¥æº":
                filename = os.path.basename(source)
            else:
                filename = "æœªçŸ¥æ–‡ä»¶"
            
            # æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ
            doc_info = f"ã€æ–‡æ¡£ç‰‡æ®µ {i}ã€‘æ¥æº: {filename}, é¡µç /è¡Œæ•°: {page}"
            doc_content = doc.page_content.strip()
            
            formatted_docs.append(f"{doc_info}\n{doc_content}")
            doc_infos.append(doc_info)
        
        return "\n\n-------------åˆ†å‰²çº¿--------------\n\n".join(formatted_docs), doc_infos
    
    def ask(self, question: str, history: Optional[list] = None) -> str:
        """
        æ”¯æŒå¤šè½®å¯¹è¯å†å²çš„é—®ç­”æ¥å£
        history: List[List]ï¼Œå¦‚ [["ç”¨æˆ·é—®é¢˜1", "åŠ©æ‰‹å›ç­”1"], ["ç”¨æˆ·é—®é¢˜2", "åŠ©æ‰‹å›ç­”2"]]
        """
        print(f"\nğŸ” æ£€ç´¢é—®é¢˜: {question}")

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.retrieve_documents(question)
        context, doc_infos = self.format_documents(docs)

        # æ„å»ºå†å²å¯¹è¯å­—ç¬¦ä¸²
        history_str = ""
        if history:
            for i, (q, a) in enumerate(history, 1):
                history_str += f"ç”¨æˆ·: {q}\nåŠ©æ‰‹: {a}\n"

        # æ„å»ºæç¤ºè¯
        prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯å›ç­”ç”¨æˆ·çš„æ–°é—®é¢˜ã€‚

ã€å†å²å¯¹è¯ã€‘ï¼ˆå¦‚æœ‰ï¼‰ï¼š
{history_str}

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯ã€‘ï¼š
{context}

ã€æ–°é—®é¢˜ã€‘ï¼š
{question}

ã€ä½œç­”è¦æ±‚ã€‘ï¼š
1. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯è¿›è¡Œå›ç­”
2. åœ¨å›ç­”ä¸­å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µç¼–å·ï¼ˆå¦‚"æ ¹æ®æ–‡æ¡£ç‰‡æ®µ1"ï¼‰
3. å¦‚æœä¿¡æ¯æ¥è‡ªå¤šä¸ªç‰‡æ®µï¼Œè¯·åˆ†åˆ«å¼•ç”¨
4. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜

ã€è¯·åŸºäºä¸Šè¿°æ–‡æ¡£ä¿¡æ¯ä½œç­”ã€‘ï¼š"""
        
#         prompt = f"""
#         ä½ æ˜¯ä¸€åä¸“ä¸šçš„åŒ»å­¦AIåŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼åŸºäºä¸‹æ–¹æ£€ç´¢åˆ°çš„åŒ»å­¦æ–‡æ¡£ä¿¡æ¯ï¼Œç§‘å­¦ã€ä¸¥è°¨åœ°å›ç­”ç”¨æˆ·çš„æ–°é—®é¢˜ã€‚

# ã€å†å²å¯¹è¯ã€‘ï¼ˆå¦‚æœ‰ï¼‰ï¼š
# {history_str}

# ã€æ£€ç´¢åˆ°çš„åŒ»å­¦æ–‡æ¡£ä¿¡æ¯ã€‘ï¼š
# {context}

# ã€æ–°é—®é¢˜ã€‘ï¼š
# {question}

# ã€ä½œç­”è¦æ±‚ã€‘ï¼š
# 1. ä»…åŸºäºæ£€ç´¢åˆ°çš„åŒ»å­¦æ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”ï¼Œä¸è¦å‡­ç©ºç¼–é€ ã€‚
# 2. å›ç­”ä¸­è¯·æ˜ç¡®å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µç¼–å·ï¼ˆå¦‚â€œæ ¹æ®æ–‡æ¡£ç‰‡æ®µ1â€ï¼‰ã€‚
# 3. å¦‚ç­”æ¡ˆæ¶‰åŠå¤šä¸ªç‰‡æ®µï¼Œè¯·åˆ†åˆ«å¼•ç”¨ã€‚
# 4. å¦‚æ–‡æ¡£ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ï¼Œè¯·ç›´æ¥è¯´æ˜â€œæ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆâ€ã€‚
# 5. ä¸å¾—æä¾›å…·ä½“è¯Šæ–­æˆ–æ²»ç–—å»ºè®®ï¼Œæ‰€æœ‰å†…å®¹ä»…ä¾›åŒ»å­¦å‚è€ƒã€‚
# 6. å›ç­”åº”ç®€æ˜ã€ä¸“ä¸šã€å®¢è§‚ï¼Œé¿å…ä¸»è§‚è‡†æ–­å’Œå¤¸å¤§å…¶è¯ã€‚

# ã€è¯·åŸºäºä¸Šè¿°åŒ»å­¦æ–‡æ¡£ä¿¡æ¯ä½œç­”ã€‘ï¼š"""


        print(f"ğŸ¤– è°ƒç”¨LLMç”Ÿæˆå›ç­”...")

        try:
            response = self.llm_client.chat_with_ai_stream(prompt)
            doc_info_str = "\n".join(doc_infos)
            final_response = response + "\n\næœ¬æ¬¡æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼š\n" + doc_info_str
            return final_response
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {e}"
        
        

    def add_documents(self, file_paths: List[str]):
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°å‘é‡åº“"""
        print("ğŸ“ æ·»åŠ æ–°æ–‡æ¡£...")
        
        # è¿™é‡Œå¯ä»¥å®ç°å¢é‡æ·»åŠ æ–‡æ¡£çš„é€»è¾‘
        # ä¸ºäº†ç®€åŒ–ï¼Œé‡æ–°æ„å»ºæ•´ä¸ªå‘é‡åº“
        self._build_vectorstore()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        key = self.user_id or "global"
        vectorstore = self._vectorstores.get(key)
        
        stats = {
            "user_id": self.user_id,
            "data_path": self.data_path,
            "vectorstore_exists": vectorstore is not None,
            "document_count": len(vectorstore.docstore._dict) if vectorstore else 0,
            "embedding_model": self.embedding.model_name,
        }
        
        return stats

    def _get_cache_path(self) -> str:
        """è·å–ç¼“å­˜è·¯å¾„"""
        key = self.user_id or "global"
        cache_dir = os.path.join("vector_cache", key)
        os.makedirs(cache_dir, exist_ok=True)
        return cache_dir
    
    def _get_vectorstore_cache_path(self) -> str:
        """è·å–å‘é‡åº“ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_dir = self._get_cache_path()
        return os.path.join(cache_dir, "vectorstore.pkl")
    
    def _get_documents_cache_path(self) -> str:
        """è·å–æ–‡æ¡£ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_dir = self._get_cache_path()
        return os.path.join(cache_dir, "documents.pkl")
    
    def _get_data_hash(self) -> str:
        """è®¡ç®—æ•°æ®ç›®å½•çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹æ–‡ä»¶å˜åŒ–"""
        if not os.path.exists(self.data_path):
            return ""
        
        hash_md5 = hashlib.md5()
        for root, dirs, files in os.walk(self.data_path):
            for file in sorted(files):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'rb') as f:
                        hash_md5.update(f.read())
                except:
                    pass
        
        return hash_md5.hexdigest()
    
    def _get_cache_hash_path(self) -> str:
        """è·å–ç¼“å­˜å“ˆå¸Œæ–‡ä»¶è·¯å¾„"""
        cache_dir = self._get_cache_path()
        return os.path.join(cache_dir, "data_hash.txt")
    
    def _load_cached_vectorstore(self) -> bool:
        """å°è¯•åŠ è½½ç¼“å­˜çš„å‘é‡åº“"""
        vectorstore_path = self._get_vectorstore_cache_path()
        documents_path = self._get_documents_cache_path()
        hash_path = self._get_cache_hash_path()
        
        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not (os.path.exists(vectorstore_path) and os.path.exists(documents_path) and os.path.exists(hash_path)):
            print("ğŸ“‹ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°æ„å»ºå‘é‡åº“")
            return False
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å‘ç”Ÿå˜åŒ–
        current_hash = self._get_data_hash()
        try:
            with open(hash_path, 'r') as f:
                cached_hash = f.read().strip()
            
            if current_hash != cached_hash:
                print("ğŸ“‹ æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°æ„å»ºå‘é‡åº“")
                return False
        except:
            print("ğŸ“‹ æ— æ³•è¯»å–ç¼“å­˜å“ˆå¸Œï¼Œéœ€è¦é‡æ–°æ„å»ºå‘é‡åº“")
            return False
        
        # åŠ è½½ç¼“å­˜çš„å‘é‡åº“
        try:
            print("ğŸ“‹ åŠ è½½ç¼“å­˜çš„å‘é‡åº“...")
            
            with open(vectorstore_path, 'rb') as f:
                vectorstore = pickle.load(f)
            
            with open(documents_path, 'rb') as f:
                documents = pickle.load(f)
            
            # é‡æ–°åˆ›å»ºæ£€ç´¢å™¨
            retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
            
            # å­˜å‚¨åˆ°å†…å­˜
            key = self.user_id or "global"
            self._vectorstores[key] = vectorstore
            self._retrievers[key] = retriever
            
            print(f"âœ… æˆåŠŸåŠ è½½ç¼“å­˜çš„å‘é‡åº“ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def _save_vectorstore_cache(self, vectorstore: FAISS, documents: List[Document]):
        """ä¿å­˜å‘é‡åº“åˆ°ç¼“å­˜"""
        try:
            print("ğŸ’¾ ä¿å­˜å‘é‡åº“åˆ°ç¼“å­˜...")
            
            # ä¿å­˜å‘é‡åº“
            vectorstore_path = self._get_vectorstore_cache_path()
            with open(vectorstore_path, 'wb') as f:
                pickle.dump(vectorstore, f)
            
            # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
            documents_path = self._get_documents_cache_path()
            with open(documents_path, 'wb') as f:
                pickle.dump(documents, f)
            
            # ä¿å­˜æ•°æ®å“ˆå¸Œ
            hash_path = self._get_cache_hash_path()
            current_hash = self._get_data_hash()
            with open(hash_path, 'w') as f:
                f.write(current_hash)
            
            print("âœ… å‘é‡åº“ç¼“å­˜ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        cache_dir = self._get_cache_path()
        if os.path.exists(cache_dir):
            import shutil
            shutil.rmtree(cache_dir)
            print(f"ğŸ—‘ï¸  å·²æ¸…é™¤ç¼“å­˜: {cache_dir}")
        
        # æ¸…é™¤å†…å­˜ä¸­çš„å‘é‡åº“
        key = self.user_id or "global"
        if key in self._vectorstores:
            del self._vectorstores[key]
        if key in self._retrievers:
            del self._retrievers[key]


# å…¨å±€å®ä¾‹
_global_rag_system: Optional[RAGSystem] = None


def get_rag_system(user_id: Optional[str] = None) -> RAGSystem:
    """è·å–RAGç³»ç»Ÿå®ä¾‹"""
    global _global_rag_system
    
    if user_id is None:
        if _global_rag_system is None or _global_rag_system.user_id is not None:
            _global_rag_system = RAGSystem()
        return _global_rag_system
    else:
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„å®ä¾‹
        return RAGSystem(user_id) 